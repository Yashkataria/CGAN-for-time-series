{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "WGAN1 for Conditional GAN for TS Generation using spectrogram array.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uniform-pierre"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "import json\n",
        "import os\n",
        "import sklearn"
      ],
      "id": "uniform-pierre",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "japanese-grill",
        "outputId": "75aa7bf0-e5c9-400d-b625-5a5bf38c77e9"
      },
      "source": [
        "tf.config.experimental.list_physical_devices()"
      ],
      "id": "japanese-grill",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innocent-wisdom"
      },
      "source": [
        "# DATA PREPERATION"
      ],
      "id": "innocent-wisdom"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transsexual-people"
      },
      "source": [
        "Interpolating cross-section data"
      ],
      "id": "transsexual-people"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "found-fortune"
      },
      "source": [
        "#with open('D:\\\\BTP\\\\temp\\\\usable_data_generated', 'r') as fout:\n",
        "#    usable_data_generated = json.load(fout)\n",
        "    \n",
        "with open('usable_data', 'r') as fout:\n",
        "    usable_data = json.load(fout)\n",
        "    \n",
        "# usable_data += usable_data_generated"
      ],
      "id": "found-fortune",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trying-exposure"
      },
      "source": [
        "Preparing cross-section data by stacking elastic,ionization and excitation cross sections.\n",
        "\n",
        "Data is converted into log and then normalize between -1 and 1 using min max"
      ],
      "id": "trying-exposure"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dramatic-fancy",
        "outputId": "1f7a7939-af36-4519-96ca-2068450d968d"
      },
      "source": [
        "import math\n",
        "\n",
        "elastic_crosssection = []\n",
        "ionization_crosssection = []\n",
        "excitation_crosssection = []\n",
        "\n",
        "for species in usable_data:   \n",
        "    elastic_crosssection.append(species['elastic']['crosssection'])\n",
        "    ionization_crosssection.append(species['ionization']['crosssection'])\n",
        "    excitation_crosssection.append(species['excitation']['crosssection'])\n",
        "\n",
        "elastic_crosssection = np.array(elastic_crosssection)\n",
        "ionization_crosssection = np.array(ionization_crosssection)\n",
        "excitation_crosssection = np.array(excitation_crosssection)\n",
        "crosssection_data = np.stack([elastic_crosssection,ionization_crosssection,excitation_crosssection], axis=2)\n",
        "crosssection_data = np.swapaxes(crosssection_data,1,2)\n",
        "\n",
        "# Converting data into log scale\n",
        "crosssection_data = crosssection_data + 10**-25\n",
        "crosssection_data = np.log(crosssection_data)\n",
        "\n",
        "# Normalizing between -1 and 1 by min max\n",
        "\n",
        "min_crosssection = np.amin(crosssection_data,axis=(0,2),keepdims=True)\n",
        "\n",
        "max_crosssection = np.amax(crosssection_data,axis=(0,2),keepdims=True)\n",
        "\n",
        "crosssection_data = 2*(crosssection_data - min_crosssection)/(max_crosssection - min_crosssection) - 1\n",
        "\n",
        "print(\"Shape of min_crosssection :\",min_crosssection.shape)\n",
        "print(\"Shape of max_crosssection :\",max_crosssection.shape)\n",
        "print(\"Shape of crosssection_data :\",crosssection_data.shape)"
      ],
      "id": "dramatic-fancy",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of min_crosssection : (1, 3, 1)\n",
            "Shape of max_crosssection : (1, 3, 1)\n",
            "Shape of crosssection_data : (46, 3, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA6yXxgNPOLB",
        "outputId": "0efd5e55-b5fe-4ced-c4d2-d2a3a305b25f"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "id": "JA6yXxgNPOLB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minimal-accreditation"
      },
      "source": [
        "Preparing spectogram of cross section data obtained from above"
      ],
      "id": "minimal-accreditation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exempt-burst"
      },
      "source": [
        "elastic_spectogram = []\n",
        "ionization_spectogram = []\n",
        "excitation_spectogram = []\n",
        "\n",
        "for i in range(crosssection_data.shape[0]):\n",
        "    \n",
        "    nfft = 100\n",
        "    noverlap = 50\n",
        "    nperseg = nfft\n",
        "    \n",
        "    specgram = signal.spectrogram( crosssection_data[i,0,:],fs=1,nfft=nfft,noverlap=noverlap,nperseg=nperseg,mode=\"psd\")\n",
        "    specgram = specgram[2]\n",
        "    specgram = np.log(specgram)\n",
        "    specgram = (specgram - np.min(specgram))/(np.max(specgram)-np.min(specgram))\n",
        "    specgram = np.flip(specgram)\n",
        "    elastic_spectogram.append(specgram)\n",
        "    \n",
        "    specgram = signal.spectrogram( crosssection_data[i,1,:],fs=1,nfft=nfft,noverlap=noverlap,nperseg=nperseg,mode=\"psd\")\n",
        "    specgram = specgram[2]\n",
        "    specgram = np.log(specgram)\n",
        "    specgram = (specgram - np.min(specgram))/(np.max(specgram)-np.min(specgram))\n",
        "    specgram = np.flip(specgram)\n",
        "    ionization_spectogram.append(specgram)\n",
        "    \n",
        "    specgram = signal.spectrogram( crosssection_data[i,2,:],fs=1,nfft=nfft,noverlap=noverlap,nperseg=nperseg,mode=\"psd\")\n",
        "    specgram = specgram[2]\n",
        "    specgram = np.log(specgram)\n",
        "    specgram = (specgram - np.min(specgram))/(np.max(specgram)-np.min(specgram))\n",
        "    specgram = np.flip(specgram)\n",
        "    excitation_spectogram.append(specgram)\n",
        "    \n",
        "        \n",
        "elastic_spectogram = np.array(elastic_spectogram)[:,:-1,:] # [:,:-1,:] is done for changing the shape from 5001 to 5000\n",
        "ionization_spectogram = np.array(ionization_spectogram)[:,:-1,:]\n",
        "excitation_spectogram = np.array(excitation_spectogram)[:,:-1,:]"
      ],
      "id": "exempt-burst",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "innovative-annotation",
        "outputId": "a3aed9d9-9633-47f7-f12c-432d1dae5ac3"
      },
      "source": [
        "elastic_spectogram[0].shape"
      ],
      "id": "innovative-annotation",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "invisible-courage"
      },
      "source": [
        "#for i in range(46):\n",
        "#    plt.imshow(elastic_spectogram[i,:,:],aspect=\"auto\")\n",
        "#    plt.show()"
      ],
      "id": "invisible-courage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seventh-address"
      },
      "source": [
        "# WGAN1 FOR GENERATING SPECTOGRAMS"
      ],
      "id": "seventh-address"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "checked-examination"
      },
      "source": [
        "noise_dim = 10\n",
        "BATCH_SIZE = 10\n",
        "train_images = elastic_spectogram\n",
        "train_images = train_images.reshape(train_images.shape[0], *(50,1,1)).astype(\"float32\")\n",
        "np.random.shuffle(train_images)\n",
        "D = 64"
      ],
      "id": "checked-examination",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accurate-aside"
      },
      "source": [
        "def make_discriminator1():\n",
        "    input_img = layers.Input(shape=(50,1,1))\n",
        "    \n",
        "    x = layers.Reshape((50,1))(input_img)\n",
        "    \n",
        "    x = layers.Conv1D(D, 5, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = layers.Conv1D(D*2, 5, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = layers.Conv1D(D*4, 5, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = layers.Conv1D(D*8, 5, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "#     x = layers.Conv1D(D*16, 5, strides=2, padding='same')(x)\n",
        "#     x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "#     x = layers.Conv1D(D*32, 5, strides=2, padding='same')(x)\n",
        "#     x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1)(x)\n",
        "        \n",
        "    discriminator1 = keras.models.Model(input_img, x, name=\"discriminator1\")\n",
        "    return discriminator1\n",
        "\n",
        "discriminator1 = make_discriminator1()\n",
        "discriminator1.summary()"
      ],
      "id": "accurate-aside",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worth-mechanics"
      },
      "source": [
        "D = 32\n",
        "\n",
        "def make_generator1(noise_dim):\n",
        "    noise = layers.Input(shape=(noise_dim))\n",
        "        \n",
        "#     x = layers.Dense(10,use_bias=False)(noise)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "#     x = layers.Dense(20,use_bias=False)(noise)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "#     x = layers.Dense(40,use_bias=False)(noise)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "#     x = layers.Dense(50,use_bias=False)(noise)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "#     x = layers.Dense(50,use_bias=False)(noise)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "        \n",
        "#     x = layers.Dense(50,use_bias=False)(noise)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"tanh\")(x)\n",
        "    \n",
        "    x = layers.Dense(1*32*D, use_bias=False)(noise)\n",
        "    x = layers.Reshape((1,32*D))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Conv1D(16 * D, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Conv1D(8 * D, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "#     x = layers.Cropping1D(cropping=1)(x)\n",
        "        \n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Conv1D(4 * D, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "#     x = layers.Cropping1D(cropping=1)(x)\n",
        "    \n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Conv1D(2 * D, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Cropping1D(cropping=1)(x)\n",
        "\n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Conv1D(1 * D, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    \n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Conv1D(D, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Cropping1D(cropping=3)(x)    \n",
        "    \n",
        "    x = layers.Conv1D(1, 5, padding='same')(x)\n",
        "    x = layers.Activation(\"tanh\")(x)\n",
        "    \n",
        "    x = layers.Reshape((50,1,1))(x)\n",
        "    \n",
        "    generator1 = keras.models.Model(noise, x, name=\"generator1\")\n",
        "    return generator1\n",
        "\n",
        "generator1 = make_generator1(noise_dim)\n",
        "generator1.summary()"
      ],
      "id": "worth-mechanics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sophisticated-operations"
      },
      "source": [
        "WGAN-GP model - copied from https://keras.io/examples/generative/wgan_gp/"
      ],
      "id": "sophisticated-operations"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accepting-crash"
      },
      "source": [
        "class WGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        latent_dim,\n",
        "        discriminator_extra_steps=5,\n",
        "        gp_weight=10.0,\n",
        "    ):\n",
        "        super(WGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_steps = discriminator_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "        super(WGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # Get the interpolated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        # 3. Calculate the norm of the gradients.\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # For each batch, we are going to perform the\n",
        "        # following steps as laid out in the original paper:\n",
        "        # 1. Train the generator and get the generator loss\n",
        "        # 2. Train the discriminator and get the discriminator loss\n",
        "        # 3. Calculate the gradient penalty\n",
        "        # 4. Multiply this gradient penalty with a constant weight factor\n",
        "        # 5. Add the gradient penalty to the discriminator loss\n",
        "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
        "\n",
        "        # Train the discriminator first. The original paper recommends training\n",
        "        # the discriminator for `x` more steps (typically 5) as compared to\n",
        "        # one step of the generator. Here we will train it for 3 extra steps\n",
        "        # as compared to 5 to reduce the training time.\n",
        "        for i in range(self.d_steps):\n",
        "            # Get the latent vector\n",
        "            random_latent_vectors = tf.random.normal(\n",
        "                shape=(batch_size, self.latent_dim)\n",
        "            )\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Generate fake images from the latent vector\n",
        "                fake_images = self.generator(random_latent_vectors, training=True)\n",
        "                # Get the logits for the fake images\n",
        "                fake_logits = self.discriminator(fake_images, training=True)\n",
        "                # Get the logits for the real images\n",
        "                real_logits = self.discriminator(real_images, training=True)\n",
        "\n",
        "                # Calculate the discriminator loss using the fake and real image logits\n",
        "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "                # Calculate the gradient penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "            # Get the gradients w.r.t the discriminator loss\n",
        "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            # Update the weights of the discriminator using the discriminator optimizer\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(d_gradient, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        # Train the generator\n",
        "        # Get the latent vector\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Generate fake images using the generator\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            # Get the discriminator logits for fake images\n",
        "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "            # Calculate the generator loss\n",
        "            g_loss = self.g_loss_fn(gen_img_logits)\n",
        "\n",
        "        # Get the gradients w.r.t the generator loss\n",
        "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generator using the generator optimizer\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(gen_gradient, self.generator.trainable_variables)\n",
        "        )\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ],
      "id": "accepting-crash",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "danish-sheet"
      },
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=6, latent_dim=noise_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "\n",
        "        for i in range(self.num_img):\n",
        "            img = generated_images[i].numpy()\n",
        "            plt.imshow(img[:,:,0],aspect=\"auto\")\n",
        "            #plt.savefig(\"D:\\\\BTP\\\\elastic_generated\\\\{idx}_{epoch}.jpg\".format(idx=i,epoch=epoch))"
      ],
      "id": "danish-sheet",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aware-deposit"
      },
      "source": [
        "# Instantiate the optimizer for both networks\n",
        "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
        "generator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "discriminator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "\n",
        "# Define the loss functions for the discriminator,\n",
        "# which should be (fake_loss - real_loss).\n",
        "# We will add the gradient penalty later to this loss function.\n",
        "def discriminator_loss(real_img, fake_img):\n",
        "    real_loss = tf.reduce_mean(real_img)\n",
        "    fake_loss = tf.reduce_mean(fake_img)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "\n",
        "# Define the loss functions for the generator.\n",
        "def generator_loss(fake_img):\n",
        "    return -tf.reduce_mean(fake_img)\n",
        "\n",
        "\n",
        "# Set the number of epochs for trainining.\n",
        "epochs = 1000\n",
        "\n",
        "# Instantiate the customer `GANMonitor` Keras callback.\n",
        "\n",
        "## UNCOMMENT FOR COMPLETE CODE\n",
        "cbk = GANMonitor(num_img=1, latent_dim=noise_dim) \n",
        "\n",
        "# Instantiate the WGAN model.\n",
        "wgan = WGAN(\n",
        "    discriminator=discriminator1,\n",
        "    generator=generator1,\n",
        "    latent_dim=noise_dim,\n",
        "    discriminator_extra_steps=5,\n",
        ")\n",
        "\n",
        "# Compile the WGAN model.\n",
        "wgan.compile(\n",
        "    d_optimizer=discriminator_optimizer,\n",
        "    g_optimizer=generator_optimizer,\n",
        "    g_loss_fn=generator_loss,\n",
        "    d_loss_fn=discriminator_loss,\n",
        ")\n",
        "\n",
        "# Start training the model.\n",
        "wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"
      ],
      "id": "aware-deposit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "described-feeding"
      },
      "source": [
        "Sample generated spectogram from \"generator1\""
      ],
      "id": "described-feeding"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "fixed-detroit",
        "outputId": "14156977-0597-48a4-b57c-ee5782fbd5dc"
      },
      "source": [
        "random_vector = tf.random.normal(shape=(1,noise_dim))\n",
        "sample_image = generator1(random_vector,training=False)\n",
        "plt.imshow(sample_image[0,:,:,0],aspect=\"auto\")"
      ],
      "id": "fixed-detroit",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f21b5462eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dynamic-metadata"
      },
      "source": [
        "Saving and loading model"
      ],
      "id": "dynamic-metadata"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "underlying-spank",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb4f34f-7c1d-41e6-e707-a80339efe440"
      },
      "source": [
        "generator1.save(\"generator_model1_1000epoch\")"
      ],
      "id": "underlying-spank",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: generator_model1_1000epoch/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stopped-arrival"
      },
      "source": [
        ""
      ],
      "id": "stopped-arrival",
      "execution_count": null,
      "outputs": []
    }
  ]
}